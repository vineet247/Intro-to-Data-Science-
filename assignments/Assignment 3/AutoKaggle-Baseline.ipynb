{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AgrdCLpEl5N4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5af42018a84d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# uploaded = files.upload()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Add all relevant imports here\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.naive_bayes as naive_bayes\n",
    "import sklearn.linear_model as linear_model\n",
    "\n",
    "from sklearn import impute\n",
    "from sklearn import preprocessing as preproc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score, log_loss, mean_squared_error, mean_absolute_error, roc_curve, auc\n",
    "\n",
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hVstbfAemBxK"
   },
   "outputs": [],
   "source": [
    "# Accessing Google sheets\n",
    "!pip install --upgrade -q gspread\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "import gspread\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
    "\n",
    "worksheet = gc.open('AutoKaggle').worksheet('Metadata')\n",
    "\n",
    "# get_all_values gives a list of rows\n",
    "_rows = worksheet.get_all_values()\n",
    "\n",
    "# Convert to a DataFrame and render.\n",
    "import pandas as pd\n",
    "rows = pd.DataFrame.from_records(_rows)\n",
    "\n",
    "new_header = rows.iloc[0] #grab the first row for the header\n",
    "rows = rows[1:] #take the data less the header row\n",
    "rows.columns = new_header #set the header row as the df header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImeUr018mDxJ"
   },
   "outputs": [],
   "source": [
    "def alpha_to_number(alpha_key):\n",
    "  return sum([(ord(alpha)-64)*(26**ind) for ind, alpha in enumerate(list(alpha_key)[::-1])]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7mlr50ymaKB"
   },
   "outputs": [],
   "source": [
    "def parseMetaData(row_id):\n",
    "  \n",
    "  # Parse data from MetaData for each row\n",
    "  column_key = {'name': 'C', 'columns': 'W', 'estimator_func_call': 'AU', 'target_name': 'AC', 'output_type': 'AA', 'performance_metric': 'BB', 'feature_selector': 'AL'}\n",
    "  column_key = dict(map(lambda kv: (kv[0], alpha_to_number(kv[1])), column_key.items()))\n",
    "  \n",
    "  metadata['competition_name'] = rows.loc[row_id][column_key['name']]\n",
    "  metadata['estimator'] = rows.loc[row_id][column_key['estimator_func_call']]\n",
    "  metadata['target_column'] = rows.loc[row_id][column_key['target_name']]\n",
    "  metadata['output_type'] = rows.loc[row_id][column_key['output_type']].split(',')\n",
    "  metadata['metric'] = rows.loc[row_id][column_key['performance_metric']]\n",
    "  metadata['feature_selector'] = rows.loc[row_id][column_key['feature_selector']]\n",
    "  columns = rows.loc[row_id][column_key['columns']]\n",
    "\n",
    "  # Parse column information \n",
    "  numeric_columns = []\n",
    "  unwanted_columns = []\n",
    "  categorical_columns = []\n",
    "  columns_data = [x.strip() for x in columns[1:-1].split(';')]\n",
    "  for ind, val in enumerate(columns_data):\n",
    "    if ind%3 == 2:\n",
    "      if (val == \"numeric\" or val == \"integer\" or val == \"real\"):\n",
    "        numeric_columns.append(columns_data[ind-1])\n",
    "      elif val == \"categorical\":\n",
    "        categorical_columns.append(columns_data[ind-1])\n",
    "      elif val == \"unwanted\" or val == \"string\" or val == 'dateTime':\n",
    "        unwanted_columns.append(columns_data[ind-1])\n",
    "    else:\n",
    "      pass\n",
    "    \n",
    "  metadata['numeric_columns'] = numeric_columns\n",
    "  metadata['unwanted_columns'] = unwanted_columns\n",
    "  metadata['categorical_columns'] = categorical_columns\n",
    "  \n",
    "  # Remove target from features columns\n",
    "  if metadata['target_column'] in metadata['numeric_columns']:\n",
    "    metadata['numeric_columns'].remove(metadata['target_column'])\n",
    "  if metadata['target_column'] in metadata['categorical_columns']:\n",
    "    metadata['categorical_columns'].remove(metadata['target_column'])\n",
    "  if metadata['target_column'] in metadata['unwanted_columns']:\n",
    "    metadata['unwanted_columns'].remove(metadata['target_column'])\n",
    "  \n",
    "  print(metadata['competition_name'])\n",
    "  print(metadata['numeric_columns'])\n",
    "  print(metadata['categorical_columns'])\n",
    "  print(metadata['unwanted_columns'])\n",
    "  print(metadata['target_column'])\n",
    "  print(metadata['metric'])\n",
    "  print(metadata['feature_selector'])\n",
    "  print(metadata['estimator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hm1kNv9qmoh2"
   },
   "outputs": [],
   "source": [
    "# We have put some fixed values here for feature_selectors,estimators etc\n",
    "# TODO: Perform reading cells for the following objects from Metadata sheet\n",
    "# TODO: Sometimes you may need to add more objects so your pipeline works efficiently\n",
    "# TODO: The data in the metadata sheet might not be completely correct, so correct it accordingly\n",
    "# TODO: The data in the metadata sheet might not be completely sufficient, let us know so wen can add new column. DO NOT ADD NEW COLUMNS ON YOUR OWN\n",
    "\n",
    "# You can see that the column W i.e `columns [index;name;type;...] for type use categorical or numerical has this information` has the type information.\n",
    "# But on closer observation this might not be completely useful. For example the type for 'Sex' can be string(that is how the data is in the dataset) but for \n",
    "# our needs its a categorical value on which we need to do one hot encoding\n",
    "# Similarly the passengerid is a numeric value which is like a unique id. We don't do normalization on it\n",
    "# Because of this reason we need to modify this column appropriately\n",
    "\n",
    "# For now we decided the types of columns will be as follows\n",
    "# numeric(integer, real) - represented as `numerical` - These columns will be normalized\n",
    "# categorical - represented as `categorical` - These values will be encoded using one-hot encoding\n",
    "# string - represted as `string`\n",
    "# dateTime - represented as `dateTime`\n",
    "# If new types are required let us know. So everyone will be using similar tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 69258,
     "status": "ok",
     "timestamp": 1555896332489,
     "user": {
      "displayName": "Divyansh Khanna",
      "photoUrl": "https://lh6.googleusercontent.com/-ai7TE5M_7qM/AAAAAAAAAAI/AAAAAAAAAEo/vjeNX_rFbzk/s64/photo.jpg",
      "userId": "08670235296186319619"
     },
     "user_tz": 240
    },
    "id": "s25lnX0emrk1",
    "outputId": "4ec24a44-9ed6-42e8-f8b5-fdc3b142e70b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /gdrive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ATHkPGgHmty1"
   },
   "outputs": [],
   "source": [
    "def preprocessing(train_df):\n",
    "  \n",
    "  # drop unwanted columns\n",
    "  if metadata['unwanted_columns']:\n",
    "    train_df.drop(metadata['unwanted_columns'], axis=1, inplace=True)\n",
    "  \n",
    "  X = train_df.drop(metadata['target_column'], 1)\n",
    "  y = train_df[metadata['target_column']]\n",
    "  \n",
    "  # treat missing values\n",
    "  pd.set_option('mode.chained_assignment', None) # used to subside the panda's chain assignment warning\n",
    "  imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "  for col in metadata['numeric_columns']:\n",
    "    X[[col]] = imp.fit_transform(X[[col]])\n",
    "    \n",
    "  # Categorial transform  \n",
    "  for col in metadata['categorical_columns']:\n",
    "    col_dummies = pd.get_dummies(X[col], dummy_na=True)\n",
    "    X = pd.concat([X, col_dummies], axis=1)\n",
    "  X.drop(metadata['categorical_columns'], axis=1, inplace=True)\n",
    "  \n",
    "  # Feature normalization\n",
    "  X[metadata['numeric_columns']] = preproc.scale(X[metadata['numeric_columns']])\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "  \n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQ0lsMYHm3Mz"
   },
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "def feature_extraction():\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyZcSB3cm5L8"
   },
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "def feature_selection(X_train, X_test, y_train, y_test):\n",
    "  \n",
    "  selector = eval(metadata['feature_selector'])\n",
    "  X_train = selector.fit_transform(X_train, y_train)\n",
    "  X_test = selector.fit_transform(X_test, y_test)\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yrDr_Qiom6xU"
   },
   "outputs": [],
   "source": [
    "def estimation(X_train, X_test, y_train, y_test):\n",
    "  \n",
    "  model = eval(metadata['estimator'])\n",
    "    \n",
    "  model.fit(X_train, y_train)\n",
    "  predict = model.predict(X_test)\n",
    "  if metadata['metric'] == \"rmse\":\n",
    "    error = np.sqrt(mean_squared_error(y_test, predict))\n",
    "  elif metadata['metric'] == \"accuracy\":\n",
    "    error = accuracy_score(y_test, predict)\n",
    "  elif metadata['metric'] == \"auc\":\n",
    "    fpr, tpr, _ = roc_curve(y_test, predict)\n",
    "    error = auc(fpr, tpr)\n",
    "  print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9UmYmuaQm83o"
   },
   "outputs": [],
   "source": [
    "# Postprocessing\n",
    "def postprocessing():\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4083
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8797,
     "status": "ok",
     "timestamp": 1555896351772,
     "user": {
      "displayName": "Divyansh Khanna",
      "photoUrl": "https://lh6.googleusercontent.com/-ai7TE5M_7qM/AAAAAAAAAAI/AAAAAAAAAEo/vjeNX_rFbzk/s64/photo.jpg",
      "userId": "08670235296186319619"
     },
     "user_tz": 240
    },
    "id": "cLucc0D-m-uw",
    "outputId": "eab0a878-ee53-4e1f-c8d3-0ff3ddd85108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'parseMetaData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9b62ca762fd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"************************************************************\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m   \u001b[0mparseMetaData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m   \u001b[0mcompetition_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'competition_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/data/trainData.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parseMetaData' is not defined"
     ]
    }
   ],
   "source": [
    "row_ids = [449, 297, 369, 6, 507, 508, 509, 510, 511, 512, 513, 514, 515, 142]\n",
    "metadata = {}\n",
    "\n",
    "# Set your current working directory\n",
    "cwd = \"/gdrive/My Drive/Introduction to Data Science Spring 2019 Term Project/Baseline/\"\n",
    "\n",
    "for row_id in row_ids:\n",
    "  \n",
    "  # Parsing MetaData updates the metadata dict\n",
    "  metadata.clear()\n",
    "  print(\"************************************************************\")\n",
    "  parseMetaData(row_id)\n",
    "  competition_dir = cwd + metadata['competition_name'] + '/data/trainData.csv'\n",
    "\n",
    "  train_df = pd.read_csv(competition_dir)\n",
    "\n",
    "  X_train, X_test, y_train, y_test = preprocessing(train_df)\n",
    "  if metadata['feature_selector']:\n",
    "     X_train, X_test, y_train, y_test = feature_selection(X_train, X_test, y_train, y_test)\n",
    "  estimation(X_train, X_test, y_train, y_test)\n",
    "  print(\"************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9E013XrnAiv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AutoKaggle-Baseline.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
